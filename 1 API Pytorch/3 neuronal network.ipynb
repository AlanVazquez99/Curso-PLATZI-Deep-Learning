{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596663033220",
   "display_name": "Python 3.7.6 64-bit ('ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels*2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_channels*2, n_channels*4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_channels*4*8*8, self.n_channels*4) # Entrada la salida de n_dims de foward\n",
    "        self.fc2 = nn.Linear(self.n_channels*4, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Empieza 3x64x64\n",
    "        x = self.conv1(x) # n_channels x 64 x 64\n",
    "        x = functional.relu(functional.max_pool2d(x, 2)) # max_pool divide dimension de imagen | n_chanels x 32 x 32\n",
    "        \n",
    "        x = self.conv2(x) # n_channels*2 x 32 x 32\n",
    "        x = functional.relu(functional.max_pool2d(x, 2)) # n_chanels*2 x 16 x 16\n",
    "        \n",
    "        x = self.conv3(x) # n_channels*4 x 16 x 16\n",
    "        x = functional.relu(functional.max_pool2d(x, 2)) # n_chanels*4 x 8 x 8\n",
    "\n",
    "        # Flatten, aplanar datos a una dimension\n",
    "        x = x.view(-1, self.n_channels*4*8*8)\n",
    "        \n",
    "        # Fully Connected (FC)\n",
    "        x = self.fc1(x) # out=n_cannels*4\n",
    "        x = functional.relu(x) \n",
    "        x = self.fc2(x) # Salida cantidad de clases a predecir\n",
    "\n",
    "        # log_SoftMax\n",
    "        x = functional.log_softmax(x, dim=1) # Salida en forma de probabilidad\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from plot_helpers import imshow\n",
    "\n",
    "import os \n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignsDataset(Dataset):\n",
    "    def __init__(self, base_dir, split_name='train', transform=None):\n",
    "        path = os.path.join(base_dir, '{}_signs'.format(split_name))\n",
    "        files = os.listdir(path)\n",
    "\n",
    "        self.filenames = [ os.path.join(path, file) for file in files if file.endswith('.jpg') ]\n",
    "        self.targets = [ int(file[0]) for file in files ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.filenames[index])\n",
    "        image = self.transform(image) if self.transform else image\n",
    "        return image, self.targets[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = SignsDataset('../datasets/64x64_SIGNS')\n",
    "\n",
    "display(len(signs))\n",
    "display(signs[0][1])\n",
    "display(signs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_set = SignsDataset('../datasets/64x64_SIGNS', split_name='train', transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(train_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    print(targets)\n",
    "    out = make_grid(inputs)\n",
    "    imshow(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(32).to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMetric():\n",
    "    def __init__(self):\n",
    "        self.summatory = 0\n",
    "        self.n_data = 0\n",
    "\n",
    "    def update(self, val, size):\n",
    "        self.summatory += val\n",
    "        self.n_data += size \n",
    "\n",
    "    def __call__(self):\n",
    "        return self.summatory/float(self.n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    print('_'*10, '\\n')\n",
    "\n",
    "    running_loss = RunningMetric() # perdida\n",
    "    running_acc = RunningMetric() # precision\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward() # magias: gradientes calculados automaticamente\n",
    "        optimizer.step() # magia2: actualiza las pesos\n",
    "\n",
    "        batch_size = inputs.size()[0]\n",
    "        running_loss.update(loss.item()*batch_size, batch_size)\n",
    "        running_acc.update(torch.sum(preds == targets).float(), batch_size)\n",
    "\n",
    "    print( 'Loss:  {:.4f}      Acc: {:.3%}'.format(running_loss(), running_acc()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}